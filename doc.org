#+STARTUP: indent
#+options: toc:nil author:nil 
#+title:
#+DATE:
#+latex_header: \usepackage[UTF8]{ctex}
#+latex_header: \setCJKmainfont{宋体}
#+latex_header: \usepackage[a4paper]{geometry}
#+latex_header: \geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}
#+latex_header: \usepackage{fancyhdr}
#+latex_header: \usepackage{nopageno}
#+latex_header: \renewcommand{\baselinestretch}{1.0}
#+latex_header: \setminted{tabsize=4,breaklines=true,frame=lines,framesep=2mm,fontsize=\small}

#+BEGIN_SRC latex
  \pagestyle{fancy}
  \lhead{\kaishu 中国科学院大学}
  \chead{}
  \rhead{\kaishu 2020年秋季学期~计算机体系结构研讨课}

  \begin{center}
    {\LARGE \bf 实验~13~报告}\\
  \end{center}
  \begin{flushright}
    { \heiti
      学号：2018K8009918009~2018K8009915034~2018KXXXXXXXXXX \\
      姓名：梁苏叁~李龙成~陈彦帆\\
      箱子：7 \\
    }
  \end{flushright} 
#+END_SRC


* 问题介绍


* 模型与算法
我们的德扑AI DeepFool 采用了蒙特卡洛虚拟遗憾最小化（MCCFR）与神经网络
结合的方法，在一轮迭代中，用MCCFR与一个策略网络交互，计算遗憾值产生带
标注的训练样本，再交给策略网络学习，通过反复迭代收敛到一个较优的策略。
在实际测试中，通过将环境信息输入策略网络来得到决策的概率分布。

** MCCFR
** 策略网络
// 梁老师

** 牌力计算
// 陈老师

* 实验细节
** 预训练策略网络
** 基于CFR的决策历史学习
** 多模型集成
* 结果与讨论
** =neuron_pocker= 环境测试结果
** 存在问题与讨论
*** 模型的收敛问题
*** 
* 参考文献


