#+STARTUP: indent
#+options: toc:nil author:nil 
#+title:
#+DATE:
#+latex_header: \usepackage[UTF8]{ctex}
#+latex_header: \setCJKmainfont{宋体}
#+latex_header: \usepackage[a4paper]{geometry}
#+latex_header: \geometry{left=2.0cm,right=2.0cm,top=2.5cm,bottom=2.5cm}
#+latex_header: \usepackage{fancyhdr}
#+latex_header: \usepackage{nopageno}
#+latex_header: \renewcommand{\baselinestretch}{1.0}
#+latex_header: \setminted{tabsize=4,breaklines=true,frame=lines,framesep=2mm,fontsize=\small}

#+BEGIN_SRC latex
  \pagestyle{fancy}
  \lhead{\kaishu 中国科学院大学}
  \chead{}
  \rhead{\kaishu 2020年秋季学期~计算机体系结构研讨课}

  \begin{center}
    {\LARGE \bf 实验~13~报告}\\
  \end{center}
  \begin{flushright}
    { \heiti
      学号：2018K8009918009~2018K8009915034~2018KXXXXXXXXXX \\
      姓名：梁苏叁~李龙成~陈彦帆\\
      箱子：7 \\
    }
  \end{flushright} 
#+END_SRC


* 问题介绍


* 模型与算法
我们的德扑AI DeepFool 采用了蒙特卡洛虚拟遗憾最小化（MCCFR）与神经网络
结合的方法，在一轮迭代中，用MCCFR与一个策略网络交互，计算遗憾值作为带
标注的训练样本，交给策略网络学习，通过反复迭代收敛到一个较优的策略。
在实际测试中，通过将环境信息输入策略网络来得到不同决策的概率分布并依照概率分布做出决策。

** MCCFR

** 策略网络
由于德州扑克的状态空间极大，单纯地使用CFR进行决策学习需要巨大的存储空
间保存每种状态的决策概率分布，因此我们通过使用策略网络学习MCCFR在不同
输入状态下的决策概率分布，用策略网络表示CFR的决策概率分布，实现对信息的压缩。

决策网络整体结构为多层感知机，网络包含三个输入分支，分别为玩家的手牌、当前的公共牌以及所有玩家之前的历史决策，输出为取每种决策（弃牌、跟注、加注等等）的概率，每层感知机包含全连接层和ReLU激活层，模型在输出前会通过SoftMax层将采取每种决策的概率进行归一化，为了防止网络过拟合在将牌信息和历史信息进行合并时使用了Dropout操作，模型的整体结构如下图所示：

TODO 网络结构图

由于扑克共有52张牌，包含4种花色，每种花色包含13张大小不同的牌，让模型直接判断手牌的大小和花色关系较为困难，因此我们将每张牌根据花色和大小进行编码构造为_embeddings_后再输入网络。具体实现中，手牌和公共牌会分别进行编码再叠加在一起作为策略网络的牌信息。

在德州扑克的对局过程中，往往需要通过其他玩家的历史决策信息调整自己的动作决策，因此我们在决策网络中引入了玩家的历史决策信息，希望模型借助已有的历史信息更好地学习动作决策。

由于策略网络需要根据输入信息拟合不同决策的概率分布，因此在约束函数部分，我们选择使用KL（Kullback-Leibler）散度度量模型输出的概率分布与MCCFR提供的概率分布之间的差异，以要求模型尽可能拟合MCCFR的概率分布。约束函数如下所示：
$$ l(X,Y) = \sum_{i=1}^{n}(y_n*(\log{y_n}-x_n)) $$
X为模型输出的概率分布（为包含ｎ维的向量），Y为MCCFR给出的概率分布（也为包含n维的向量）。

** 牌力计算
// 陈老师

* 实验细节
** 预训练策略网络
由于MCCFR的策略学习是基于多次迭代的增量式学习，因此需要通过大量的迭代才能让模型学到正确的决策，所以在有限的算力下直接使用MCCFR进行迭代学习收敛速度太慢。为了加速训练过程，我们使用基于规则的Agent在给定牌型下进行决策，生成大量的“信息、决策”样本对，决策网络首先在样本对上进行反复学习从而具备与规则Agent相同的决策能力，实现决策网络的预训练过程，再在此网络基础上和MCCFR进行交互学习，从而应对更复杂的决策情况。

规则Agent在生成“信息、决策”样本对时，将随机初始化手牌和公共牌（公共牌可能部分不可见）信息，通过牌力计算器计算出当前的获胜概率，以获胜概率为中心构造高斯分布，不同的动作依据自身的冒险程度采样高斯分布得到做出该决策的概率。


** 基于CFR的决策历史学习

** 多模型集成

* 结果与讨论

** =neuron_pocker= 环境测试结果
** 存在问题与讨论
*** 模型的收敛问题
*** 

* 参考文献
