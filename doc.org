#+STARTUP: indent
#+SETUPFILE: env.org
#+options: toc:nil author:nil
#+title:
#+DATE:

#+BEGIN_SRC latex
  \pagestyle{fancy}
  \lhead{\kaishu 中国科学院大学}
  \chead{}
  \rhead{\kaishu 2020年秋季学期~人工智能基础}

  \begin{center}
    {\LARGE \bf 德州扑克AI报告}\\
  \end{center}
  \begin{center}
    { \heiti
      姓名：梁苏叁~李龙成~陈彦帆\\
      学号：2018K8009918009~2018K8009915034~2018K8009918002 \\
      \url{https://github.com/Cothrax/deepfool}
    }
  \end{center} 
#+END_SRC


* 问题介绍
德州扑克属于不完美信息博弈的一种，一直被人工智能学界作为测试解决不完美
信息博弈的算法的基准游戏之一，多种基于CFR的方法已经成功解决了2人的德州
扑克AI问题，但6人无限注的德扑AI依然是一个open的问题。在本次大作业中，
我们结合的博弈树搜索和深度学习的方法，设计了6人无限注的德扑AI DeepFool，
并在 =neuron_poker= 环境对模型进行了测试。

* 模型与算法
DeepFool 采用了蒙特卡洛虚拟遗憾最小化（MCCFR）与神经网络结合的方法，在
一轮迭代中，用MCCFR与一个策略网络交互，计算遗憾值产生带标注的训练样本，
再交给策略网络学习，通过反复迭代收敛到一个较优的策略。在实际测试中，通
过将环境信息输入策略网络来得到决策的概率分布。

** MCCFR
算法的主要框架采用了虚拟遗憾最小化（CFR）的方法，寻找一个理论上为纳什
均衡的博弈策略。我们将策略定义为一个已知信息 $I$ 到合法行动的概率分布
$\pi$ 的映射 $\sigma:I\to\pi$ ，已知信息 $I$ 包含了对于当前玩家的所有
可见信息，如手牌、公共牌、决策历史等，概率分布 $\pi$ 的每一项 $\pi_a$
代表在已知 $I$ 的情况下采取行动 $a$ 的可能性。

CFR是一种求解纳什均衡策略的高效算法，它的一次迭代产生一颗博弈树，博弈
树上的所有玩家都根据 策略 $\sigma$ 行动，每个节点 $u$ 都会得到一个每个
玩家 $p$ 的期望收益 $u_{u,p}$ ，对于终止节点，收益即为游戏结束后赢得的
筹码；对于非终止节点，收益为该节点的所有子节点的收益的概率加权
(ref:eq:cfr2)。树上的一个节点已知信息对应了映射 $\sigma$ 中的一项 $I$
，在这个节点上一个合法行动 $a$ 的遗憾值 (ref:eq:cfr3) 等于以概率1采取
行动 $a$ 的期望收益减去 $u$ （严格来讲，还需要乘上从根到该节点上，其他
玩家决策概率贡献的乘积作为系数），每次迭代遗憾值累加后再归一化就得到了
$I$ 下的决策概率分布。CFR被证明可以在 $O(N^2)$ （ $N$ 状态空间大小）内
收敛到一个纳什均衡的策略 cite:neller2013introduction 。
 
#+BEGIN_SRC latex
  \begin{align}
    \label{eq:cfr1} v &=  \textsc{Next-State}(u, a) \\
    \label{eq:cfr2} \textsc{Util}_{u,p} &= \sum_{a\in \textsc{Action}(u)} \pi_a\cdot \textsc{Util}_{v,p} \\
    \label{eq:cfr3} \textsc{Regret}_{u,a} &\propto \textsc{Util}_{v, p}-\textsc{Util}_{u,p}
  \end{align}
#+END_SRC


但在6人的德州扑克中，直接使用朴素的CFR会带来两个问题：
1) 博弈树太大，比如在 =neuron_poker= 环境中，环境实际只允许每轮加注1次，
   这意味着最多有8轮迭代，环境允许的行动有6种，则一次完整的迭代需要访
   问 $6^{6\times 8}$ 个节点，这依照当代计算机的算力是不现实的。
2) 状态空间太大，即 $\sigma$ 里的项数太多，需要大量的迭代才能收敛到一
   个较好的策略。

针对这两个问题，我们的模型对朴素的CFR做出了两点改进。
1) 对于第1个问题，我们采取的蒙特卡洛采样（MC）的方法，具体来讲，对于每
   次迭代，固定一个当前玩家 $P$ ，只在 $P$ 决策的节点展开子树，在其余
   玩家决策的节点直接根据决策概率采样，这样搜索的规模被缩小到了
   $6\times 6^6$ 。可以证明当采样次数足够多时，每个节点采样得到效用的期望
   等于实际的效用值 cite:burch2012efficient 。
2) 对于第2个问题，我们用一个策略神经网络来拟合决策映射 $\sigma$ ，替代
   朴素CFR采用的表格法，CFR在搜索时通过询问策略网络来遍历博弈树并计算
   遗憾值，用计算的遗憾值更新得到的节点的决策，将这些新决策作为产生的
   训练样本交给策略网络学习。

** 策略网络
由于德州扑克的状态空间极大，单纯地使用CFR进行决策学习需要巨大的存储空
间保存每种状态的决策概率分布，因此我们通过使用策略网络学习MCCFR在不同
输入状态下的决策概率分布，用策略网络表示CFR的决策概率分布，实现对信息
的压缩。

决策网络整体结构为多层感知机，网络包含三个输入分支，分别为玩家的手牌、
当前的公共牌以及所有玩家之前的历史决策，输出为取每种决策（弃牌、跟注、
加注等等）的概率，每层感知机包含全连接层和ReLU激活层，模型在输出前会通
过SoftMax层将采取每种决策的概率进行归一化，为了防止网络过拟合在将牌信
息和历史信息进行合并时使用了Dropout操作，模型的整体结构如图
ref:fig:arch 所示。

#+ATTR_LaTeX: :width 1\linewidth :placement [!htpb]
#+caption: 策略网络结构图 label:fig:arch
[[./architecture.png]]

由于扑克共有52张牌，包含4种花色，每种花色包含13张大小不同的牌，让模型
直接判断手牌的大小和花色关系较为困难，因此我们将每张牌根据花色和大小进
行编码构造为 =embeddings= 后再输入网络。具体实现中，手牌和公共牌会分别
进行编码再叠加在一起作为策略网络的牌信息。

在德州扑克的对局过程中，往往需要通过其他玩家的历史决策信息调整自己的动
作决策，因此我们在决策网络中引入了玩家的历史决策信息，希望模型借助已有
的历史信息更好地学习动作决策。

由于策略网络需要根据输入信息拟合不同决策的概率分布，因此在约束函数部分，
我们选择使用KL（Kullback-Leibler）散度度量模型输出的概率分布与MCCFR提
供的概率分布之间的差异，以要求模型尽可能拟合MCCFR的概率分布。约束函数
如下所示：

#+BEGIN_SRC latex
  \begin{equation}
    l(X,Y) = \sum_{i=1}^{n}(y_n\cdot(\log{y_n}-x_n))
  \end{equation}
#+END_SRC

$X$ 为模型输出的概率分布（为包含 $n$ 维的向量），$Y$ 为MCCFR给出的概率
分布（也为包含 $n$ 维的向量）。

** 牌力计算
1) 确定牌力计算：在判断胜负的阶段，需要比较每个玩家7张牌的牌力大小。具
   体实现时，先考虑给定的排好序的5张牌的大小。依次检查同花、顺子、四条、
   葫芦、三条、两对、一对、高牌，以及牌面数字，计算出牌力大小，以一个
   整数表示。为了提高速度，预先计算好所有13选5组合的牌型大小（考虑同花
   和非同花两种情况）。对于给定的7张牌，考虑所有21种可能的牌型情况，返
   回其中最大的5张牌的牌力。
2) 基于蒙特卡洛采样的期望牌力计算：在游戏进行过程中，需要根据已知的底
   牌和公共牌信息可以得到当前玩家的牌面胜率。未知牌的组合情况较多，采
   用蒙特卡洛模拟方法。具体来说，随机采样未知的牌，计算所有玩家的牌力
   大小，得到当前玩家本轮模拟的胜负情况。重复多次模拟，取平均值，得到
   当前玩家的期望胜率。

为加快速度，牌力计算用C++实现，得到的结果按需缓存到哈希表中。
* 实验细节
** 预训练策略网络
由于MCCFR的策略学习是基于多次迭代的增量式学习，因此需要通过大量的迭代
才能让模型学到正确的决策，所以在有限的算力下直接使用MCCFR进行迭代学习
收敛速度太慢。为了加速训练过程，我们使用基于规则的Agent在给定牌型下进
行决策，生成大量的“信息、决策”样本对，决策网络首先在样本对上进行反复
学习从而具备与规则Agent相同的决策能力，实现决策网络的预训练过程，再在
此网络基础上和MCCFR进行交互学习，从而应对更复杂的决策情况。

规则Agent在生成“信息、决策”样本对时，将随机初始化手牌和公共牌（公共
牌可能部分不可见）信息，通过牌力计算器计算出当前的获胜概率，以获胜概率
为中心构造高斯分布，不同的动作依据自身的冒险程度采样高斯分布得到做出该
决策的概率。

** 基于CFR的决策历史学习
在得到一个初步可用的网络后，再采用MCCFR来生成决策样本，但因为使用MCCFR
时每一个节点都需要询问一次神经网络，这样生成样本的效率偏低，所以这一部
分分为两个子步骤：
1. 通过直接采样得到大量的样本进行训练：想得到一次迭代的搜索树结构，只
   需要知道采样的节点选择了哪个行动，在这一个子步骤，MCCFR在采样时不询
   问策略网络，而直接随机选择一个节点，这样可以在完成整个搜索树的遍历
   之后，将所有要询问的样本交给策略网络，可以大大加快迭代速度。
2. 但子步骤1的做法违背了MCCFR的原理，在理论上是不收敛的，所以在直接采
   样了一定轮数之后，还是采用标准的MCCFR进行搜索树的遍历，伪代码如下：

   #+BEGIN_SRC latex
     \begin{algorithm}[caption={MCCFR训练决策网络伪代码}, label={code:mccfr}]
     Input: Current state $game$, sampling player $player$, strategy network $network$
     Output: Utility for current state $game$

     Function $\textsc{MCCFR}$($game, player, network$) Begin
         If $\textsc{Status}(game) = \textsc{GAME\_OVER}$ Then
             Return $\textsc{Payoff}(game)$

         $strategy \gets$ $\textsc{Get-Strategy}(network, game)$
         If $\textsc{Player}(game)=player$ Then Begin
             $action \gets \textsc{Sample-Action}(strategy)$
             $next \gets \textsc{Take-Action}(game, action)$
             Return $\textsc{MCCFR}(next, player, network)$
         End Else Begin
             $cvUtil \gets [0, 0, \cdots, 0]$
             $util \gets [0, 0,  \cdots, 0]$
             For $action \in$ $\textsc{Legal-Action}(game)$ Begin
                 $next \gets \textsc{Take-Action}(game, action)$
                 $nextUtil \gets \textsc{MCCFR}(next, player, network)$
                 $cvUtil_{action} \gets nextUtil_{player}$
                 $util \gets util + strategy_{action}\cdot nextUtil$
             End
             $regret \gets cvUtil - util_{player}$
             $\textsc{Update-Strategy}(network, regret)$
             Return $Util$
         End
     End
     \end{algorithm}
   #+END_SRC

* 结果与讨论
** =neuron_pocker= 环境测试结果
** 存在问题与讨论
*** 模型的收敛问题
虽然使用神经网络的方法压缩了决策映射的，但因为MCCFR的学习决策历史时每
次都需要询问策略网络，这导致迭代的速度变得非常慢，我们在训练时采用了并
行多个MCCFR的方法，即每个子进程遍历一颗博弈树，主进程将生成的regret整
合成新的样本。尽管如此，由于我们笔记本有限的算力和训练时间，模型没能完
全收敛，所以得到的决策网络鲁棒性不好。这是未来可以进一步优化的方向。
*** =neuron_poker= 环境的问题
在测试中，我们发现 =neuron_pocker= 实际存在如下问题：
1) 大作业的要求为设计6人无限注德州扑克AI，但环境中的
   =is_raising_allowed= 函数每轮只允许加注6次，也就是说环境中游戏最多
   进行8圈（每轮一圈加注，一圈跟注）
2) 环境中的历史信息没有严格的时序关系，只有在某一轮每个人是否
   raise/check，也没有提供fold的决策历史和有多少人出局的信息，这给决策
   历史部分网络的设计带来的很大的困扰。

nocite:moravcik17_deeps

bibliography:citations.bib
bibliographystyle:plain
